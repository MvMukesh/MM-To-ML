{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQpVgl_dUfUF"
   },
   "source": [
    "# <center> <u> Feature Selection </u> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What's the Purpose of Feature Selection</h2>\n",
    "<p>Many learning algorithms perform poorly on high-dimensional data. This is known as the <b>curse of dimensionality</b>\n",
    "    <p>There are other reasons we may wish to reduce the number of features including:\n",
    "        <p>1. Reducing computational cost\n",
    "            <p>2. Reducing the cost associated with data collection\n",
    "                <p>3. Improving Interpretability\n",
    "                    \n",
    "Reference for entire topic-\n",
    "\n",
    "https://www.youtube.com/watch?v=EqLBAmtKMnQ"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAChCAYAAABTYaNVAAAf1UlEQVR4Ae3dLZMstxXGcXN/gIsuMDO7wGauMjIxuDDYzCQsLDwkLNBVQUH5AAFmgUExCwtMVUBYWNikfuN9ruX2zO72tKSel6Oq2e5pSUfq50j/Pv0yvR8dKpUCpUApcKcKfHSn+1W7VQqUAqXAoQBXg6AUKAXuVoEC3A269ve///3h7du39ZmoAc0r3Z4CBbjb89nBZHv37t3hT3/60+GPf/xjfQZqQGNaF+BucKIcDnWKeotuM9m+/PLLW+z6TfaZ1gW4m3RdAe4W3VaAm+u1AtxcvXu2VqeoPdWcZKsAN0nop2YKcHP17tlaAa6nmpNsFeAmCf3UTAFurt49WyvA9VRzkq0C3CShn5opwM3Vu2drBbieak6yVYCbJPRTMwW4uXr3bK0A11PNSbYKcJOEfmqmADdX756tFeB6qjnJVgFuktBPzRTg5urds7UCXE81J9kqwE0S+qmZAtxcvXu2VoDrqeYkWwW4SUI/NVOAm6t3z9YKcD3VnGSrADdJ6KdmCnBz9e7ZWgGup5qTbI0G3L/+9a/D119/ffjoo48On3766XH9q6++Ov4m8w9/+MPFe/nf//734rp7VizA7an+trYLcNv026X2aMDZqf/973+Hb7755vC3v/3twz4ClB/3X5LU/e677y6punudAtzuLri4AwW4i6Xbr+IswP36178+/PWvfz3u6H/+85/DP/7xj4PlJen7778//OY3v7mk6u51CnC7u+DiDhTgLpZuv4ozAffnP//5IPr6y1/+cgCpJBGe09X3798ffvWrXx3++c9/HrMsv/322+PbTkSATnd/+OGHwxdffHH4+OOPj1Hh7373u+OpbyI6+/PmzZujfQCVL1JkH2Slv//978d17ak383S3ABev396yAHd7Pju+usekG5kATMQFUoADYiAnyfvtb397AD8RnnL68+9///sIoUR9ygRi4BhYsQFe7fW82AcyMJQHluoBpDasa9N1QctZqQA3S+n+7RTg+ms63OLMCC7X4MCmXQc+IMtHnqhKOVEYGLkxEYiB4znAAWYARzxg9KLJJJAE2bRlqZ1ZqQA3S+n+7RTg+ms63OJMwIHJMrkWZ9KfOk3MqaR6oJQI7jWAA0Up0WHaZSegzLaZywLcTLX7tlWA66vnFGuzAOda2inAAZsIzvUw+aAm4rK0zXU3qQUTwLEnqZ+ozHcRX6K99vT3WPhwOLbh+h3IaYOtU/1K+d7LAlxvRefZK8DN07pbS6MBBziiKFBx6pjIqt0BUdznn39+fFbO83LKAJdrZUDmu36q79pcoj4gZD/flQErEAHEXGNTL6ehoAeI2vFhPxBt+zRqvQA3Stnxdgtw4zXu3sJowAEKKAGR5alTUTtluzI+SbalDjttnvXWlu/KKmd7lmnb96TYUqfdnvyRywLcSHXH2i7AjdV3iPXRgBvS6Rs2WoC7XecV4G7QdwW4uU4rwM3Vu2drBbieak6yVYCbJPRTMwW4uXr3bK0A11PNSbZGAM51LRf1fXI97KVrXe5m5tcKPXbdA71uLrTX7XrY3WqjALdVwf3qF+D20/7ilkcAziMY+eWBO53uWrrY/1xSp30s5Lmyp/IA1N3QgBTY3H3N91N19thWgNtD9T5tFuD66DjVSm/AebZMJBawgA54vfQohvwtEZxIEUzT7lQRVzRWgFsh1pUVLcBdmUNe053egPMc2rt3747PpqV9p6rtqSIYAVq7TZkl4E6VY3O5HdQ89+bnW+z47qNcm5b1kme78vqjX9ZHpQLcKGXH2y3Ajde4ewu9AQcOQOWNHk5Nl5EbAHkAV1TnQV7X3qQl4Jbl8oCw7erqt1Na21PXD+uBznf5+hHIiSxtk2+7N4zoq/qgo0/64rqdcqNSAW6UsuPtFuDGa9y9BZPZpOuZREIA4pcCbAdOgCLKAhFwAyTRnutz7SkqKPmFARAp51cOPuwCVH6oz678gMp1vySnqwGcesr5xYOUX06AnpsRftqVPtrmDcRsjkgFuBGqzrFZgJujc9dWegOuBQNQAdLbt2+PUAKy/DRKuXzsUKIwZawDofWUsQQqcLR9mURfAKecBFza0gdAXN5RBTwRpvJAGPipB7zaGpEKcCNUnWOzADdH566t9AYc0IBEm0BORAcaoqP2x+22gUwLONGcconU2FLOB3xyWms7gKm/BJxIDODk6Q977ekywDktVR/g0ucCXOu5Wm8VKMC1atzIem/AOdUTLQUmoCQaC6yA5ZNPPjlGW8DnAzLKpx4oqeNllCmjn8qJ0my3lOd6nO05XQVKbQJVTlHlgx0bkghQnqU87QZwIrmK4G5k8E7uZgFusuA9musNONEZmImO3AQAj1zf0l/wASfX5+QDC6BZd2MCeHwHHzBUjo2cQrb1WzABJBt5JEXExh4bknzrThHBkz3tgKQ3nSRi1L7vymird6pT1N6KzrNXgJundbeWegOuW8fu1FAB7nYdW4C7Qd8V4OY6rQA3V++erRXgeqo5yVYBbpLQT80U4Obq3bO1AlxPNSfZulXAuabmGtqW5AbDqUdOtth8qW4B7iWFrje/AHe9vjnbs1GAAw83Gjwe4ibDORi5IaFMHtk429GnDHY8EuJObG48vFTnVD5AuqHQPjt3qlzvbQW43orOs1eAm6d1t5ZGAQ6IPLLh7qQHfU/BCGQ8kqGMsucgaGfd0YwNZf2yId8vEUNb7rgW4C5R7zHrFOBu0O+jABcp/GQKwIBkmURtfialDy8lZfO4iejQIyFbAKe95cPBL/WhR35FcD1U3MdGAW4f3Te1OhpwwARyIjWRVxJIaTufbLcEMvXycLCliM1zbJ6zc90sgPMcnbKiwSTRmXK2t7+akK9d9sFNfiI4dWxXnq0Rz8BpvwAXL93esgB3ez47AsakG5VARKQFTm2kFsiAn4dtpUBGeT+1Ap/Ay/Uy9eWBj4d8Xbvz3akm++pLvgdu6imXxKY2QdPDvAFcrgUCJlvsjkgFuBGqzrFZgJujc9dWQGM04EREAOLNIdaBCNQswSeAAxd9cUorQrMecAGRqEtanqKKDJ3q2i4C87OsJHmfffbZEVigyk5SIjjfQTE/vtfXNtpM+R7LAlwPFfexUYDbR/dNrc4CnE6KpkAEhAKrFnBLAGXHgPAlwDmFderKnsgsSV2w1CaQit6S2mtwwKt/IKyMeiNSAW6EqnNsFuDm6Ny1lZmAAxkAEZXlGhcg6YMkcgIqyySnqiKzlwDHLpugJQIMoCzzm1VRWqJF9gM46yI2gNS28qK7EakAN0LVOTYLcHN07trKSMCBC6A49ZR8F02J4iTg0r7ISZ7v4OcH9p5z80lZdgAq1+fctABMCQTzKAobAKWsdVGhuuyDmDeR2CYPNLUFdKI2sLVdmwW4o7T1p1GgANeIcSurowAHFGwDFpAEcgDldFACkuSDkDpABDTqgIzvEjgpI8LKtbN8147ygVrbdntKyo5+pCwwglvglxsbyzrHDnT6UxFcJyF3MFOA20H0rU2OAtzWft1r/QLc7Xq2AHeDvivAzXVaAW6u3j1bK8D1VHOSrQLcJKGfminAzdW7Z2sFuJ5qTrJVgJsk9FMzBbi5evdsrQDXU81Jtgpwk4R+aqYAN1fvnq0V4HqqOclWAW6S0E/NFODm6t2ztQJcTzUn2SrATRL6qZkC3Fy9e7ZWgOup5iRbBbhJQj81U4Cbq3fP1gpwPdWcZKsAN0nop2YKcHP17tlaAa6nmpNsFeAmCf3UTAFurt49WyvA9VRzki2A86qhSnMUeO0bjOf0plpZo0ABbo1aV1DW7zq9hcMP1b3F45Y+fmzvc0t91tc3b94cfwt7Be6vLqxUoAC3UrC9ivuxux+q+3G6H5Zb3tLHj/S9McTH+i31XV/92F+/89KBvcZBtbtOgQLcOr12Ke1NHN7g4c0Zt5yAeeRbP0Zq4+0lXvXkxZxe3VTpNhQowF2xn7xCKC9yzCuIrri7L3bNK5VEQbec8lonoJv9D6hvWbe9+l6A20v5Z9r1llsw8FLJezolugfAxW3ekcc/omvQq3SdChTgrswvbiI4HTVx7i3dE+D4RlTtlNv1OX6rdH0KFOCuxCeiADcRwO1eI4J7A1yGjlNVkOO/StelQAFuZ3+IAtw8cK3t3qOAewVchpCbD/xoeQ/XTLNft7wswO3oPdfX8vhB/mPVjt0Z3vS9A46Aom83Uvh11P9pHe6oO2qgALeTM127cZH63qO2Vt5HAFz2Nzchbv2ucfbnVpcFuMmey8A32R8tPRLg4lv/Acxpq2iuTlujyrxlAW6S1u2pC8g9YnpEwPFz+yuUe3rs5xbGcAFugpcAzVHck/CPfBR/VMBliPG/yxL1S4goMn5ZgBuo8SM8+rFGvkcHXLTyOIlfQtRNiCgyblmAG6CtKC2/W7Ss9KMCBbifRoKoHuTqlxA/aTJirQDXWVWPe3hY12Su3yr+XNwC3M/1EOG7CfHtt98+1N30n6sw9lsBrqO+uWP2qDcRXpKyAHdaofwSwrNzj3yN9rQ627YW4Lbpd6ztlwgGp/eGVTqvQAHuvDZy3Hxw2upAWaB7XqvX5hbgXqvUiXLtox91wfiEQItNBbiFICe+Zky5zFFj6oRAKzcV4FYKluIGXz36ETVetyzAvU4npVzLNb5oVtHc63VblizALRV54buB55++OMI+wu9HX5BjVXYBbpVcx8Lusnp2znXdAt16/QpwKzQz2Fxrc42k0noFCnDrNVPDrx8cVD0/VwfVdRoW4F6hl+sibuW7iWC90mUKFOAu0y21vJjh/fv3dYCNIK9YFuCeEQnM8uhHXfB9RqhXZhXgXinUC8XyS4h6HOkFoQ6HQwHujEYGj9NRrzWqqO2MSCs3F+BWCvZM8Rqfz4jTZBXgGjGympsIFbVFkT7LAlwfHWPFgddPAd1t/eGHH7K5lo0CBbhGDEdF1zjqbQ+NKB1XC3AdxWxMAZ2zDZ8622iEqVPUH8Vwlyr/vbx+P/rzAdLzWwGup5q/tCWa8/jSPf5Htl/u7eu2PHwEJ1rznFG99eN1A2ZLqQLcFvVeV9djJHkdfl1ieeCbDKI2R7v6V2+vmzg9ShXgeqj4OhtOVV2be/SbZA8XwXG8qM1zbXWb/XWTpVcpkw3kKs1TwOmqH/A/0j83atV9KMABWt7WUBdj22Ewbt1pkmcJc32I/tZtq1Oocbq3lvNLCDchHu2XEHcHuHPgchPB5KpJ1Q798esm1EcffXT4+OOPf/ax7dEm23i1n28h/2D83E8Nz82d561ed+5dAc6PkU2c9pkg0QKw1Z2l/Qai65z80n7q2ud+/sgTA+0lGnPmzZs3d/eD/rsCXCZSIjUP7PoI0Svtp4Co+ZNPPvkAOOsVSe/nDy2Dm/kCdh6NMmccgHy/p3Q3gHME+vTTT49Ocjr0+eef1ytmrmikOtAkgrNeaX8FnPE4wzFXzBn+MYfayG7/Xm7rwV0AjqM88pEJlOU9XlPY5u79aosSvvjii+OnHqbezw/Lls2RzJcs3Yy4l3fP3QXgHIXinHYJepWuR4Hvvvvu4FPpehQ4FRiYQ/fyWMkHwHk+yXn4LX6++uqrw9u3b4/XeVzfyUe47Xm3a92nc3ezXhr+ebbpWvfrXL88eOpzLv+at196k4qPr3W/zI2cmraBgfXPPvvsavv9kp7ts5YfACcsBQSV7fitfc7t9DXuh766Y3VpNOMaFojfqq+u0Sfn+kRjWl963ZCP+fpafaVf5z7nNLnW7fYDw7As6WeAc3StNEcBk2YL4EStleYoQOstgOPrSnMUwLAC3Bytn21lK+C+/PLLZ+1XZj8FaF2A66fnSEsFuJHqrrBdgFsh1s5FC3A7O2BF8wW4FWKNLFqAG6luX9sFuL56jrRWgBup7grbBbgVYu1ctAC3swNWNF+AWyHWyKIFuJHq9rVdgOur50hrBbiR6q6wXYBbIdbORQtwOztgRfMFuBVijSxagBupbl/bBbi+eo60VoAbqe4K2wW4FWLtXLQAt7MDVjRfgFsh1siiBbiR6va1XYDrq+dIawW4kequsF2AWyHWzkULcDs7YEXzBbgVYo0sWoAbqW5f2wW4vnqOtFaAG6nuCtsFuBVi7Vy0ALezA1Y0X4BbIdbIogW4ker2tX0vgPMSS+952/LPftT1/sVTL8T0rwHkrUmX1HnOfgHuOXUm5l0b4Aw0r5v55ptvjksTwTv7fffxP00l/0tBOQMp2ybKtktTewHO23ZbH9A8Hz5Y838tAMn/W/D+t/afMq0R1BgxFryZeQk4fdE3+cu8c21cUuecrWzXh3qbSNTYcXltgCOFAedFh3kRp4FqwHrraztoA78d5Zva9F6Ay056kwnQtYmP/BvANQkw+fNSwGlL3ffv3/9sPKQPxsUawKl3SZ20d2pZgDulyg7brhFwZDCZ2v+sZBIZ0I7eSd5ue0//mCT7dW65N+BEyi3gHGyAZu3/tnB6CUBbfKduAe7cSKntHxS4VsA5on799deH/MMeUZ3XwefaikniRZ3yrQd2wKiu5JXR3vpqUiojsZOyIkJ5acNklQemTr0Ato0Y89pvpx6xpy3b9Uv52Do21vnP3oCjZ/s+Ojp9//33H/aSBvTwoZ88+tGF1gEa7QAumrV5jNGQxmwkik8jfC4PbEVJ8Y8lXyaP35PnoBh7GT/sPVcn7V26rAjuUuU617tWwBl8AJcBaaCbFAaqPJMlg98E8Ypo300qkDKpAih1AjkD7927d8f6mUiZYN6YKyoAQXkmSSY0m/pisphc+qY9tkxEcFQ24OvspqO5vQFnv107s++08m/+4h9Lr0SnJY3ooQwg8Rf9+E+iLT8kj8/YUk80yG+xEZ3Z4Eeaq68u+7bHV8Bqna/TFl8qyx4gu26nr+fqsNcjFeB6qNjBxrUCzq4ZqD4Go8llgBrwJoGBbhJJ8g3ofLfNIAZBA9vEMPEkk8DgS5Jv0LNpoqWcfPW1Zxm4BpYmoTr6ZduMtDfglhEcLWmWBCT5RysBCO0kvqF7gETP5MmnIS35VTl+j9ba0BbfBEDs5RRVndYHfB/A6ZP11h5b5+rEfvbp0uVQwOmkI4bBK1naIaLfQuIATtqSAoGXIoprBhwdDBQDPEdd3/nSwI0/c8rTThh5IGTSmBiZALEZbY2VRG1LwLGrPXWALu2lrqUJbfLMSNcAuHZfadeOL3rzi2S7CDgHHQcnWqpDR9BJnvKiX1ryFzvLtARSC7gleAM4bemvusv0XJ1l2Uu+29eMOfVf/U9niKOikNaRV5jqY912+T45bbCTGjNAW2dc0um2DoHSfuuotgxhlTGBTJLXJM4BnVNOfk19ZTKA6BLIn6t7zYALYPgvvjOBaOoAliSvjQjsP6jlIKFsBhs/8EeSU5hMPP4yKZP41TiKfev0NKZMWHVNyJzGpt6o5d6AO7evDgw04RvzQuKDFnC05CPl5NE888a2AI/Pcq01ZfnFdnXUldTNQYd/5SWlrPryXEbgd9/5Un/P1Yn92Lp0qT8Zc2y8GnBpEADagZWdyURIOUsDkYDJc6TP4G/LrV0nmv/d6Cih/TZlUnjcYZnXlrNO1Bz5fLe+BXBsaJ/Itww4+8HHwBINDWxQbvfLgG2hR0+AU5evLaMFvzs4GuDWDcKMBRMJRDKh2AAyydLEy//tVE8f9M3YEjGPTnsBjvYBlH017n3oZuwDHy1oHC3kGfvGccY3vwU0xngOQkswshmdLfmQDZrTmw11+MNS22AqXx5/ux7Ix/wiL/Zcq1WevVN17EvG2hZ/bgacjhAiiQPsjI4vE4FawBHdpNia2CW4SMl6m0wIAmr3JcEMBs5O0r+tgKNFJnXsnlpecwSnvwajA1QSLTNJso0vQckyWqtnm3Fhncapyyf8k/zY8T0TyPrSp75rgy3jLG3Y1vYx9nov9wScfaZJPuDh47uDqf1PnnU+yndjMT6yXYovlMlBJHrJ0546rQ9oblv0Vi/+1ge21NM+v7d5sdf66bk66culy82AEwoHcHa8BYKB53t2hkgGtXKIj+b559K22VH0d+qiHmFsU5ZDtKO9CJadZpeTEy1ku6WjCMGXgFNeO+AXR4go3J0iCsdoVz+0bVu7b2zLF57Lyz7arn/KytOOtmnxXLp2wD3X90vyoumpuvzVHmhOldlz216A23Ofb7XtzYATwQlRTWITGqCSwEV4HPoHcKAlAZbBLDm6+G7gOyKkYwBh8gMGiPh+CnCOJqDkPD8w8V15yxZwwKRd9thlP0clkUOSPADUX/kiRLYk+5m+6y8byTM5k2d569fgokevJf85UDm4tQcG9uls3NBsmder/a12CnBbFZxXPxxJi6uvwRmoBmQS4CUZrMByCnAGeQsJoSuYqO8DTLYp1wIjttsl+0CirPYATNIvfQAeOyofrLSjTNrRlnJsLAEXW+qyAcAAagIG1NpST1l9SVu2K+t7oGvbqfRIERzdaB4ft3rQKXm0vsZUgLtGr5zuk7nXBl2rAQcSOUXVBFAkrQEcuLQdaW0AnAjtXArg5IMi+LCnb1IiQpAKcFo4xe4pwInipABOP7TndLa1QQNAlQ+gSWmvABdFfrmkp4MDjS9N6gLjDCg+MuBE1XzVzvO1PouvzNXRaTPglhFc2+El4IgjwgIGO5kIzrooyykLUCjnA1DyXgO4FoB2yqkqG1IbwbEn2jJIbU/EYAlwgZNy4BbAscOuduTl+lr2Vx85TJvaNmkl3wH3pYl37REcreyz66b8RoNlss+uYfq0ui3L+Z76xgJd+SPbTpV/aRu4uWPHh6PTtQOOH1xf5ivj8pSuxju9+KoNUE5pp75P5rM75VsAx0e5u3qqvZ7bLgacHTbZDXoTOBO/7ZxBR0CRjYFMSDsGipIloACDfOVzG9mEF3nJcx2Po06Jqow84LQuEVBb+sjZ8tl15OFY2zJZbc8ptkmc6I8Ng8RHef3Qd7b0FbCIZyLbD5/0TzvycgoM3Mrpz7l07YDTb/2nB41y8Gj3xz7aj5dgrg59Y4O+/PecPm0759b5ht3R6doBZ/9pSVMwou8yGa981QYGyzL5rkxs8Bm7Gesps3bpIGm+j06bAGenQcHH+nKAJt+SIO13O5ZtgCGpT0DlgEjyPW2cElW55KcOOynr5gXwpe20Zbtt7KffaZ8d5bJfyqYf6sR22rYtNrIfaU9Z5dr8444t/twK4BwMDPBlhGb/RMYOWC8Bjh7KWkr8YyC+pNFCsl98LcD9JAkt+Yg/+KzV1jqt+MCB+7lkHihn/EvG9UMA7jlRKm+9ArcCOJPGpBDFBVD2VrRqu4nTAg68TDKRX6Jc30XOInP1HEgAzuQxmURImVBsiyDUtz2ReRRWX18C3orgflQGxJxJ8MUy4hY58ZUoqgUcP9Cf1rnO5nt8ZVsApyxfixCtJ7EXX7GfYEA+3+gLHxoDVx3BZYdq2UeBWwFcBr6juHXJUR5gDGYTIoCzNMgBCoic4hvkyhngASQIumRg8kggyp6kPBuSetpNngmSdX0wsQpwR6mOERsd45NWp2wHqACOD3ynH12NRz4DSr7KAYeP6BwfO2jFP/zMhqSesZC81o985YBWgPvRVw/x91YAZ3JIBqyIygQysE2UDOoMfgBU3vd8QM0pP1Bl0phcvqsvZdLEXgstdkQN6qoTKKpnQrVlj8YG/LmVa3DxFf+48cVX/OQ7bVvAgRkI0lcZSxqDUatzIji2JJAK1MCMnSSRnacN2OEbfk5StgAXNR5geWuAM8AdhU0ig9pEsK2N4EyYRHmtC0FuCbj2GhzAmTTsKddOBJPFpDHR1PE9SZ0C3I9qAFgAFx35g5YOMEvAKZvy0dPyNYDjoxyM+C6p9ZUy7alsAS4qPcjy1gDHLWDi+kygkkHu6C+JFJyWmji2mVyigzaCM4Ec2VvAAWaiAuttnrZcwwlMMym1bRIlb+SwubUIjhZ056scLAI4Pkq+XyQliqOzvMDRAYWvcmCxXWKP7ll3jS156gOZtiz5Rso4sY3NkcnY0U7S6gd9U7GW2xS4dsA5GhsoeeTFwDSQcy3NOih5LMj1NBPENhGcieWjbOop62PiifrkKwt2JgkwZoKZGIGXCZhJoU8GsPomWgYzgI5M1w44EfLSVzkA0Ixf5PMVrWknX/QVX9E7kZ6yNOYrPlOGH8Auj1vF37a3voovErXHV8qwZfvIlDGRNgpwUWLy8toBZwIASz6+S1laT56lSZR86z5t2djLMnWW32Mjtlsb8thN3WUbxw4M+HPtgGs1pE00y5Ik0TPaRabo2ZaNvSxTt/2untRus96m2Lb0Wea3ZXutF+B6KbnRzrUDbuPu3VX1awfcXYm9cWcKcBsF7FW9ANdLyfF2CnDjNe7VQgGul5Ib7RTgNgo4sXoBbqLYG5sqwG0UsFf1AlwvJcfbKcCN17hXCwW4XkputFOA2yjgxOoFuIlib2yqALdRwF7VC3C9lBxvpwA3XuNeLRTgeim50U4BbqOAE6sX4CaKvbGpAtxGAXtVL8D1UnK8nQLceI17tVCA66XkRjsFuI0CTqxegJso9samCnAbBexVvQDXS8nxdgpw4zXu1UIBrpeSG+0U4DYKOLF6AW6i2BubKsBtFLBX9QJcLyXH2ynAjde4VwsFuF5KbrRTgNso4MTqBbiJYm9s6lnA5TXSXlRXn3EaeO3MVsDlvyeVn8b5ibZ8RWuvBbokeSUUX7NTvhrvKwzzuqekD69Lynuf8n6oWv74TrOROuTljXHGa5felzayX2X7l77Pyxtf66OU4+PS85d6jtQEy5I+AC7vbsq7n2r507vQRmlB80tS+Wq8b5Y+L1/N13zpg9d+b331AXCXTLSqUwqUAqXANStQgLtm71TfSoFSYJMCBbhN8lXlUqAUuGYF/g8unFYPZ7swEwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Filter Methods:\n",
    "\n",
    "\n",
    "\n",
    "Filter method applies a statistical measure to assign a scoring to each feature.Then we can decide to keep or remove those features based on those scores. The methods are often univariate and consider the feature independently, or with regard to the dependent variable.\n",
    "\n",
    "In this section we will cover below approaches:\n",
    "\n",
    "1. `Missing Value Ratio Threshold`\n",
    "2. `Variance Threshold`\n",
    "3. $Chi^2$ Test\n",
    "4. `Anova Test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Missing Value Ratio Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame named diabetes and load the csv file\n",
    "diabetes = pd.read_csv('8A-diabetes.csv')\n",
    "#print the head \n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that some features can not be zero(e.g. a person's blood pressure can not be 0) hence we will impute zeros with nan value in these features.\n",
    "\n",
    "Reference to impute: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.replace.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in diabetes.columns:\n",
    "    val_cnt = diabetes[col].value_counts()\n",
    "    print(val_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glucose BloodPressure, SkinThickness, Insulin, and BMI features cannot be zero ,we will impute zeros with nan value in these features.\n",
    "col_with_o = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "def impute_0(df):\n",
    "    for col in diabetes.columns:\n",
    "        if col in col_with_o:\n",
    "            df[col].replace(0,np.nan,inplace=True)\n",
    "        \n",
    "impute_0(diabetes)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "diabetes['Glucose'].replace(0,np.nan,inplace=True)\n",
    "diabetes['BloodPressure'].replace(0,np.nan,inplace=True)\n",
    "diabetes['SkinThickness'].replace(0,np.nan,inplace=True)\n",
    "diabetes['Insulin'].replace(0,np.nan,inplace=True)\n",
    "diabetes['BMI'].replace(0,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the no of null values in each feature\n",
    "diabetes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see for each feature what is the percentage of having missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#percentage of missing values\n",
    "\n",
    "for col in col_with_o:\n",
    "    null_per = (diabetes[col].isnull().sum()/diabetes.shape[0])*100\n",
    "    print('--'*20)\n",
    "    print(f'percentage of missing values for {col} is: {null_per}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#percentage of missing values for Glucose\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate the percentage for Bloodpressure\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate the percentage for SkinThickness\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate the percentage for Insulin\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate the percentage for BMI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey can you see that a large number of data missing in SkinThickness and Insulin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will keep only those features which are having missing data less than 10% as our threshold.\n",
    "\n",
    "Reference to check methods for dropping nan in pandas- https://www.youtube.com/watch?v=57vFbsiZYHg\n",
    "\n",
    "You can also check its document official: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are keep only those features which are having missing data less than 10% \n",
    "diabetes_missing_value_threshold= diabetes.dropna(thresh=int(diabetes.shape[0]*.9),axis=1)\n",
    "\n",
    "# print diabetes_missing_value_threshold\n",
    "diabetes_missing_value_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in diabetes_missing_value_threshold:\n",
    "    null_per = (diabetes_missing_value_threshold[col].isnull().sum()/diabetes.shape[0])*100\n",
    "    print('--'*20)\n",
    "    print(f'percentage of missing values for {col} is: {null_per}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now Seperate the data diabetes_missing_value_threshold into features and labels \n",
    "\n",
    "Hey buddy! label is something which is dependent on other features for its outcome. You can also called it as our Target variable which we predict using ML algorithms.\n",
    "\n",
    "Can you think which column would be considered as label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diabetes_missing_value_threshold_features = diabetes_missing_value_threshold.drop('Outcome',axis=1)\n",
    "\n",
    "diabetes_missing_value_threshold_label= diabetes_missing_value_threshold['Outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print diabetes_missing_value_threshold_features\n",
    "\n",
    "diabetes_missing_value_threshold_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print diabetes_missing_value_threshold_label\n",
    "diabetes_missing_value_threshold_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Variance Threshold\n",
    "\n",
    "`If the variance is low or close to zero, then a feature is approximately constant and will not improve performance of the model. In that case, it should be removed.`\n",
    "\n",
    "Variance will also be very low for a feature if only a handful of observations of that feature differ from a constant value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference-\n",
    "https://www.youtube.com/watch?v=uMlU2JaiOd8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv to dataframe name \"diabetes\" and print the head values\n",
    "diabetes = pd.read_csv('8B-diabetes_cleaned.csv')\n",
    "\n",
    "# display diabetes.head()\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the features and the target as x and y \n",
    "x = diabetes.drop('Outcome',axis=1)\n",
    "y = diabetes['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have seen the video then Krish must have told you to use `sklearn library to calculate variance threshold`. But `we will use var function to calculate our variance so that you understand the concept from base`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return  the variance for X along the specified axis=0.\n",
    "x.var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Hey smarty! did you see that DiabetesPedigreeFunction variance is less so it brings almost no information because it is (almost) constant , this can be a justification to remove DiabetesPedigreeFunction column but before considering this we should scale these features because they are of different scales.\n",
    "    \n",
    "Reference for minmax scaling: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "Lets use sklearn minmax scaler here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import minmax_scale\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "\n",
    "# use minmax scale with feature_range=(0,10) and columns=X.columns,to scale the features of dataframe and store them into X_scaled_df \n",
    "X_scaled_df = pd.DataFrame(minmax_scale(x,feature_range=(0,10)),columns=x.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Wait a minute! whats minmax scaling? also called [Normalization]\n",
    "\n",
    "It is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [−1, 1]\n",
    "\n",
    "hey hey heyieeee! Fun fact time:\n",
    "\n",
    "There is another scaling method called StandardScaler which follows Standard Normal Distribution (SND). Therefore, it makes mean = 0 and scales the data to unit variance. \n",
    "\n",
    "Cool right? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return X_scaled_df\n",
    "X_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again return  the variance for X along the specified axis=0 to check the scales after using minmax scaler.\n",
    "X_scaled_df.var(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now again you can check the previous video:https://www.youtube.com/watch?v=uMlU2JaiOd8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import variancethreshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# set threshold=1 and define it to variable select_features\n",
    "fetrs = VarianceThreshold(threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Impliment fit_transform on select_features passing X_scaled_df into it and save this result in variable X_variance_threshold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_variance_threshold_df= fetrs.fit_transform(X_scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Were you thinking of fit_transform?` We are here to help you understand\n",
    "\n",
    "`fit_transform() is used on the training data so that we can scale training data and also learn scaling parameters of that data.` Here, the model built by us will learn the mean and variance of the features of the training set. These learned parameters are then used to scale our test data\n",
    "\n",
    "Don't worry you will get lot of challenges to use these things in our other assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print X_variance_threshold_df\n",
    "X_variance_threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert X_variance_threshold_df into dataframe\n",
    "X_variance_threshold_df = pd.DataFrame(X_variance_threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print of head values of X_variance_threshold_df \n",
    "X_variance_threshold_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below mentioned is the function get_selected_features for returning selected_features to be used further.\n",
    "\n",
    "Warning ;)\n",
    "If we have provided you a readymade function, don't just use it but understand it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_features(raw_df,processed_df):\n",
    "    \n",
    "    selected_features=[]\n",
    "    for i in range(len(processed_df.columns)):\n",
    "        for j in range(len(raw_df.columns)):\n",
    "            if (processed_df.iloc[:,i].equals(raw_df.iloc[:,j])):\n",
    "                selected_features.append(raw_df.columns[j])\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the X_scaled_df as raw_df and X_variance_threshold_df as processed_df inside get_selected_features function\n",
    "selected_features= get_selected_features(X_scaled_df,X_variance_threshold_df)\n",
    "\n",
    "# print selected_features\n",
    "selected_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super! you can see SkinThickness feature is not selected as its variance is less.\n",
    "\n",
    "Lets give column names to our X_variance_threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define selected_features as columns and save it in variabe named X_variance_threshold_df\n",
    "X_variance_threshold_df.columns = selected_features\n",
    "#print X_variance_threshold_df\n",
    "X_variance_threshold_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Chi-Squared statistical test (SelectKBest)\n",
    "\n",
    "Chi2 is a measure of dependency between two variables. It gives us a goodness of fit measure because it measures how well an observed distribution of a particular feature fits with the distribution that is expected if two features are independent.\n",
    "\n",
    "Note: hi-Square is to be used when the feature is categorical, the target variable is any way can be thought as categorical. It measures the degree of association between two categorical variables. If both are numeric, we can use Pearson’s product-moment correlation, and if the attribute is numerical and there are two classes we can use a t-test if more than two classes we can use ANOVA.It may be noted Chi-Square can be used for the numerical variable as well after it is suitably discretized.\n",
    "\n",
    "\n",
    "Scikit-Learn offers a feature selection estimator named SelectKBest which select K numbers of features based on the statistical analysis.\n",
    "\n",
    "Reference link: https://chrisalbon.com/machine_learning/feature_selection/chi-squared_for_feature_selection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below mentioned function generate_feature_scores_df is used to get feature score for using it in  Chi-Squared statistical test explained below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_scores_df(X,Score):\n",
    "    feature_score=pd.DataFrame()\n",
    "    for i in range(X.shape[1]):\n",
    "        new =pd.DataFrame({\"Features\":X.columns[i],\"Score\":Score[i]},index=[i])\n",
    "        feature_score=pd.concat([feature_score,new])\n",
    "    return feature_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame named diabetes and load the csv file again\n",
    "diabetes = pd.read_csv('8C-diabetes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign features to X variable and 'outcome' to y variable from the dataframe diabetes\n",
    "\n",
    "X = diabetes.drop('Outcome',axis=1)\n",
    "y = diabetes.Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import chi2 and SelectKBest\n",
    "from sklearn.feature_selection import chi2,SelectKBest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data cast to a float type.\n",
    "X = X.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use SelectKBest to calculate the best feature score. Use Chi2 as Score Function and no.of feature i.e. k as 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise SelectKBest with above parameters \n",
    "chi2_test= SelectKBest(score_func=chi2,k=4)\n",
    "\n",
    "# fit it with X and Y\n",
    "chi2_model= chi2_test.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the scores of chi2_model\n",
    "chi2_model.scores_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use generate_feature_scores_df function to get features and their respective scores passing X and chi2_model.scores_ as paramter\n",
    "feature_score_df= generate_feature_scores_df(X,chi2_model.scores_)\n",
    "\n",
    "# return feature_score_df\n",
    "feature_score_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see the features and corresponding chi square scores? This is so easy right, higher the score better the feature. Just like higher the marks in assignment better the student of ours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get X with selected features of chi2_model using tranform function so we will have X_new\n",
    "X_new= chi2_model.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whaaaat! `tranform()? , how is it different from fit_transform.`You know buddy fit() can also confuse you. Hey you inquisitive learner we will tell you the difference.\n",
    "\n",
    "* `fit() function calculates values of these parameters`\n",
    "* `transform function applies values of parameters on actual data and gives normalized value` * `fit_transform() function performs both in same step`. Note that the same value is got whether we perform in 2 steps or in a single step.\n",
    "\n",
    "for more info on this refer: https://www.youtube.com/watch?v=BotYLBQfd5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_new into a dataframe\n",
    "\n",
    "X_new= pd.DataFrame(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat the previous steps of calling get_selected_features function( pass X and X_new as score in the function)\n",
    "selected_features= get_selected_features(X,X_new)\n",
    "\n",
    "# return selected_features\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let have X with all features given in list selected_features and save this dataframe in variable chi2_best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_best_features = X[selected_features]\n",
    "\n",
    "# print chi2_best_features.head()\n",
    "chi2_best_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see chi-squared test helps us to select  important independent features out of the original features that have the strongest relationship with the target feature.\n",
    "\n",
    "You did it well!\n",
    "\n",
    "As you can see chi-squared test helps us to select  important independent features out of the original features that have the strongest relationship with the target feature.\n",
    "\n",
    "You did it well!\n",
    "\n",
    "Now many learners have this confusion below on two questions frequently on this section:-\n",
    "1. How can the χ2-test work for feature selection for continuous variables here (since Glucose, Insulin, BMI and Age are continues and only target is categorical)?\n",
    "2. Is it a problem that this test is scale dependent?\n",
    "\n",
    "\n",
    "\n",
    "Mostly we get confused on the data itself (which can be continuous) with the fact that when you talk about data, you actually talk about samples, which are discrete.\n",
    "\n",
    "The χ2 test (in wikipedia and the model selection by χ2 criterion) is a test to check for independence of sampled data. I.e. when you have two (or more) of sources of the data (i.e. different features), and you want to select only features that are mutually independent, you can test it by rejecting the Null hypothesis (i.e. data samples are dependant) if the probability to encounter such a sample (under the Null hypothesis), i.e. the p value, is smaller than some threshold value (e.g., p < 0.05)\n",
    "\n",
    "So now for above two questions,\n",
    "\n",
    "1. The χ2 test do work only on categorical data, as you must count the occurences of the samples in each category to use it, but as I've mentioned above, when you use it, you actually have samples in hand, so one thing you can do is to divide your samples into categories based on thresholds (e.g., cat1:x∈[th1<x<th2],cat2:x∈[th2<x<th3], etc.) and count all the samples that fall into each category.\n",
    "2. As for the scales - you are obviously must use the same scales when you discretize your samples, otherwise it won't make any sense, but when you conduct the χ2 test itself, you are dealing with counts, so they won't have any scales anyway.\n",
    "\n",
    "\n",
    "Hope this clears your doubt. \n",
    "\n",
    "Hence you can see that in sklearn library also example of Chi2 test is given on numerical features i.e. iris data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Anova-F Test\n",
    "\n",
    "The F-value scores examine the varaiance by grouping the numerical feature by the target vector, the means for each group are significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://www.youtube.com/watch?v=9zrQ_c5RZkI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.feature_selection import f_classif,SelectPercentile\n",
    "\n",
    "\n",
    "# Initialise SelectPercentile function with parameters f_classif and percentile as 80\n",
    "Anova_test= SelectPercentile(f_classif,percentile=80)\n",
    "\n",
    "\n",
    "\n",
    "#Fit the above object to the features and target i.e X and Y\n",
    "Anova_model= Anova_test.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here you have used f_classif for Anova-F test. To know more about this test you can check this artical.\n",
    "\n",
    "https://towardsdatascience.com/anova-for-feature-selection-in-machine-learning-d9305e228476\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return scores of anova model\n",
    "Anova_model.scores_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use generate_feature_scores_df function to get features and their respective scores by passing X and Anova_model.scores_ as score in function \n",
    "\n",
    "feature_scores_df= generate_feature_scores_df(X,Anova_model.scores_)\n",
    "\n",
    "# print feature_scores_df\n",
    "feature_scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all supported columns values in Anova_model with indices=True\n",
    "cols = Anova_model.get_support(indices=True)\n",
    "# Reduce X to the selected features of anova model using tranform \n",
    "\n",
    "X_new = X.iloc[:,cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print X_new.head()\n",
    "X_new.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey brighty! Hope you learned to implement Anova F-test method for feature selection. It has selected 6 best features as you can see in above output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wrapper Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods are used to select a set of features by preparing where different combinations of features, then each combination is evaluated and compared to other combinations.Next a predictive model is used to assign a score based on model accuracy and to evaluate the combinations of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and read the diabetes.csv using pandas and print the head values\n",
    "\n",
    "diabetes = pd.read_csv('8C-diabetes.csv')\n",
    "diabetes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign features to X and target 'outcome' to Y variable(Think why the 'outcome' column is taken as the target)\n",
    "X= diabetes.drop('Outcome',axis=True)\n",
    "\n",
    "Y= diabetes.Outcome\n",
    "\n",
    "#return X,Y\n",
    "\n",
    "X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Recursive Feature Elemination\n",
    "\n",
    "Recursive Feature Elimination selects features by recursively considering smaller subsets of features by pruning the least important feature at each step. Here models are created iteartively and in each iteration it determines the best and worst performing features and this process continues until all the features are explored.Next ranking is given on eah feature based on their elimination orde. In the worst case, if a dataset contains N number of features RFE will do a greedy search for $N^2$ combinations of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "reference video: https://www.youtube.com/watch?v=MYnxxRoPiwI\n",
    "\n",
    "Note: the video is using random forest classifier, but we are going to use logistic regression as our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RFE and LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression as lor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model variable with LogisticRegression function with solver = 'liblinear'\n",
    "model= lor(solver='liblinear')\n",
    "\n",
    "# rfe variable has RFE instance with should have model and n_features_to_select=4 as parameters\n",
    "\n",
    "rfe= RFE(model,n_features_to_select=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit rfe with X and Y\n",
    "\n",
    "fit= rfe.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of selected features',fit.n_features_)\n",
    "print('Selected Features',fit.support_)\n",
    "print('Feature rankings',fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use below function to get ranks of all the features\n",
    "def feature_ranks(X,Rank,Support):\n",
    "    feature_rank=pd.DataFrame()\n",
    "    for i in range(X.shape[1]):\n",
    "        new =pd.DataFrame({\"Features\":X.columns[i],\"Rank\":Rank[i],'Selected':Support[i]},index=[i])\n",
    "        feature_rank=pd.concat([feature_rank,new])\n",
    "    return feature_rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all feature's ranks using feature_ranks function with suitable parameters. Sotre it in variable called feature_rank_df\n",
    "feature_rank_df= feature_ranks(X,fit.ranking_,fit.support_)\n",
    "\n",
    "# print feature_rank_df\n",
    "feature_rank_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are four features with rank 1 ,RFE states that these are the most significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter feature_rank_df  with selected column values as True and save result in variable called recursive_feature_names \n",
    "recursive_feature_names= feature_rank_df[feature_rank_df['Selected']==True]\n",
    "\n",
    "# print recursive_feature_names\n",
    "recursive_feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally get dataframe X with all the features selected by RFE and store this result in variable called RFE_selected_features\n",
    "RFE_selected_features= X[recursive_feature_names['Features'].values]\n",
    "\n",
    "# print RFE head()\n",
    "RFE_selected_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to watch this video to know about working of RFE:  https://www.youtube.com/watch?v=Yo1vYRdJ95k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Embedded Method using random forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.youtube.com/watch?v=em4OFr-4C34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection using Random forest comes under the category of Embedded methods. Embedded methods combine the qualities of filter and wrapper methods. They are implemented by algorithms that have their own built-in feature selection methods. Some of the benefits of embedded methods are :\n",
    "1. They are highly accurate.\n",
    "2. They generalize better.\n",
    "3. They are interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries RandomForestClassifier and SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file using pandas and print the head values\n",
    "diabetes = pd.read_csv('8C-diabetes.csv')\n",
    "\n",
    "# print diabetes.head()\n",
    "diabetes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all feature selection procedures, it is a good practice to select the features by examining only the training set. This is to avoid overfitting.\n",
    "So considering we have a train and a test dataset. We select the features from the train set and then transfer the changes to the test set later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign features to X and target 'outcome' to Y(Think why the 'outcome' column is taken as the target)\n",
    "X = diabetes.drop('Outcome',axis=True)\n",
    "\n",
    "Y = diabetes.Outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test_train_split module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting of dataset(test_size=0.3)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=108)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here We will do the model fitting and feature selection altogether in one line of code.\n",
    "\n",
    "Firstly, specify the random forest instance, indicating the number of trees.\n",
    "\n",
    "Then use selectFromModel object from sklearn to automatically select the features. Simple right?. Don't worry trust your code. It helps.\n",
    "\n",
    "Reference link to use selectFromModel: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of Select from Model. Pass an object of Random Forest Classifier with n_estimators=100 as argument. \n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "# fit sel on training data\n",
    "sel.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectFromModel will select those features which importance is greater than the mean importance of all the features by default, but we can alter this threshold if we want.\n",
    "\n",
    " To see which features are important we can use get_support method on the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sel.get_support() print the boolean values for the features selected. \n",
    "sel.get_support()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list named selected_feat with all columns which are True\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "\n",
    "# print length of selected_feat\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected_feat\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done Champ!. Let us impliment SelectFromModel using LinearSVC model also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using SelectFromModel\n",
    "\n",
    "\n",
    "SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef_ or featureimportances attribute after fitting. The features are considered unimportant and removed, if the corresponding coef_ or featureimportances values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”.\n",
    "\n",
    "Lets use selectfrommodel again with LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearSVC \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SelectFromModel with LinearSVC() as its parameter and save it in variable 'm'\n",
    "\n",
    "m = SelectFromModel(LinearSVC())\n",
    "\n",
    "#fit m with X and Y\n",
    "m.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list named selected_feat with all columns which are supported\n",
    "\n",
    "selected_feat= X_train.columns[(m.get_support())]\n",
    "\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Handling Multicollinearity with VIF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity refers to a situation in which more than two explanatory variables in a multiple regression model are highly linearly related. We have perfect multicollinearity if, for example as in the equation above, the correlation between two independent variables is equal to 1 or −1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance inflation factor measures how much the behavior (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables.\n",
    "\n",
    "VIF has big defination but for now understand that:-\n",
    "Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.youtube.com/watch?v=6JpmgzCAusI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and read the diabetes_cleaned.csv file using pandas and print the head values\n",
    "\n",
    "dia_df= pd.read_csv('8B-diabetes_cleaned.csv')\n",
    "dia_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataframe using .describe()\n",
    "dia_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see range of these features are very different that means they all are in different scales so lets standardize the features using sklearn's preprocessing scale function.\n",
    "\n",
    "Reference doc: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#iterate over all features in dia_df and scale\n",
    "for col in dia_df:\n",
    "    dia_df[[col]] = preprocessing.scale(dia_df[[col]].astype('float64'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe dataframe using .describe()\n",
    "dia_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import variance inflation factor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign features to X and target to Y \n",
    "X = dia_df.drop('Outcome',axis=1)\n",
    "Y = dia_df.Outcome\n",
    "\n",
    "# split the data to test and train with test_size=0.2\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=108)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign an empty dataframe to variable vif\n",
    "vif= pd.DataFrame()\n",
    "\n",
    "# make a new column 'VIF Factor' in vif dataframe and calculate the variance_inflation_factor for each X \n",
    "vif['VIF Factor']= [variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vif['Features'] with columns names in X\n",
    "\n",
    "vif['Features']= X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  round off all the decimal values in the dataframe to 2 decimal places for VIF dataframe and print it.\n",
    "vif.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VIF = 1: Not correlated\n",
    "* VIF =1-5: Moderately correlated\n",
    "* VIF >5: Highly correlated\n",
    "\n",
    "Glucose, Insulin, and Age are having large VIF scores, so lets drop it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to above observation , drop  'Glucose', 'Insulin' and 'Age' from X\n",
    "\n",
    "X= X.drop(['Glucose','Insulin','Age'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now again we calculate the VIF for the rest of the features\n",
    "\n",
    "Again repeat the previous steps to assign an empty dataframe() to vif and make a new column 'VIF Factor' and calculate the variance_inflation_factorfor each X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor']= [variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vif['Features'] as columns of X and return vif with round off to 2 decimal places\n",
    "vif['Features'] = X.columns\n",
    "vif.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now colinearity of features has been reduced using VIF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The need to fix multicollinearity depends primarily on the below reasons:\n",
    "\n",
    "1. When you care more about how much each individual feature rather than a group of features affects the target variable, then removing multicollinearity may be a good option\n",
    "2. If multicollinearity is not present in the features you are interested in, then multicollinearity may not be a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Feature Selection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
